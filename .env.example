# Flask
FLASK_SECRET_KEY=change-me-in-production
# CORS: set to 1 if frontend is on a different origin (e.g. separate deploy). When enabled, set CORS_ORIGIN to your frontend URL (e.g. https://app.example.com) so the session cookie is accepted; do not rely on * with credentials.
# ENABLE_CORS=0
# CORS_ORIGIN=https://localhost:5173
# Default admin password (used when creating first user)
ADMIN_PASSWORD=admin
# Auto-login as admin when frontend loads (dev/local only; set to 1 to skip login page)
# AUTO_LOGIN=1
# Session inactivity timeout in minutes (NIST/CJIS-style; default 60)
# SESSION_TIMEOUT_MINUTES=60
# Account lockout after N failed logins (NIST AC-7 / CJIS; default 5 attempts, 15 min lock)
# LOCKOUT_MAX_ATTEMPTS=5
# LOCKOUT_DURATION_MINUTES=15
# Audit log retention (separate from RETENTION_DAYS; 0 = never delete audit_log)
# AUDIT_RETENTION_DAYS=365
# Password policy: min length, require digit/special (0 to disable)
# PASSWORD_MIN_LENGTH=8
# PASSWORD_REQUIRE_DIGIT=1
# PASSWORD_REQUIRE_SPECIAL=1
# Password expiry in days (0 = no expiry; NIST/CJIS-style)
# PASSWORD_EXPIRY_DAYS=0
# Number of previous passwords to disallow reuse (0 = no history check)
# PASSWORD_HISTORY_COUNT=5
# Security: HSTS header when behind HTTPS
# STRICT_TRANSPORT_SECURITY=1
# Redirect HTTP to HTTPS when behind reverse proxy (set X-Forwarded-Proto or ENFORCE_HTTPS=1)
# ENFORCE_HTTPS=0
# CSP (e.g. default-src 'self'; script-src 'self'; style-src 'self' 'unsafe-inline')
# CONTENT_SECURITY_POLICY=default-src 'self'; script-src 'self'; style-src 'self' 'unsafe-inline'; img-src 'self' data: blob:
# MFA (NIST IR 8523 / CJIS 6.0): TOTP for operator/admin; install pyotp and set ENABLE_MFA=1
# ENABLE_MFA=0
# MFA_ISSUER_NAME=VMS

# Chain of custody / evidence: equipment identifier (NISTIR 8161). Stored in AI detection logs and exports; default hostname or vigil.
# SYSTEM_ID=site-01-dvr
# Recording fixity (OSAC/SWGDE): periodic SHA-256 of recordings, store and alert on mismatch. Set 1 to enable background fixity job.
# ENABLE_RECORDING_FIXITY=0

# YOLO object detection (see docs/YOLO_INTEGRATION.md). Default: yolov8n.pt. Use YOLO_DEVICE for GPU (e.g. 0) or cpu.
# For faster inference use an exported model: set YOLO_MODEL to path to .onnx (CPU/GPU), .engine (TensorRT), or OpenVINO dir. See docs/TECHNOLOGY_RESEARCH_AND_CONFIG.md.
# YOLO_MODEL=yolov8n.pt
# YOLO_DEVICE=
# YOLO_IMGSZ=640   # inference size (640 = faster; 1280 = heavier). Default 640.
# YOLO_CONF=0.25   # confidence threshold (0.01–0.95)
# YOLO_IGNORE_CLASSES=bird,cat   # comma-separated class names to ignore (fewer false alarms)
# YOLO_CLASS_CONF=person:0.4,car:0.5   # optional per-class confidence (or JSON {"person": 0.4})
# Optional: YOLO_EXPORT_FORMAT=onnx|engine|openvino (prefer loading exported model when file exists)
# YOLO_OPENVINO_DEVICE=intel:cpu|intel:gpu|intel:npu  (when using OpenVINO export)
# YOLO_TENSORRT_FP16=1  (Jetson/Ampere+; 0 for FP32)
# YOLO_TENSORRT_WORKSPACE_MB=4  (TensorRT build workspace)
# JETSON_MODE=0    # set 1 on Jetson for tuned defaults
# MJPEG stream: STREAM_JPEG_QUALITY=82 (1-100), STREAM_MAX_WIDTH=0 (0=no resize; 640 = lighter stream)
# STREAM_JPEG_QUALITY=82
# STREAM_MAX_WIDTH=0

# Emotion recognition: auto (DeepFace then EmotiEffLib), deepface, or emotiefflib. See docs/EMOTION_INTEGRATION.md.
# On Python 3.14 (TensorFlow unavailable), set EMOTION_BACKEND=emotiefflib and pip install emotiefflib.
# EMOTION_BACKEND=auto

# MediaPipe pose (pip install mediapipe). Used for Standing/Person down, fall detection, and gait_notes.
# With MediaPipe 0.10.30+, the app uses the Tasks API and downloads pose_landmarker_lite.task to ./models/ on first run.
# ENABLE_GAIT_NOTES=1   # 1 = use pose for gait_notes (normal, bent_torso, asymmetric); 0 = always "normal"
# MEDIAPIPE_POSE_MODEL_PATH=   # optional: path to pose_landmarker.task (default: auto-download to ./models/pose_landmarker_lite.task)
# POSE_BACKEND=mediapipe   # future: rtmpose, mmpose (requires extra deps)

# Cameras: comma-separated OpenCV indices or RTSP URLs. Use "auto" or leave unset to auto-detect laptop/device cameras.
# Set to "yaml" to load from config/cameras.yaml (or CONFIG_DIR/cameras.yaml). Requires PyYAML; see config/cameras.example.yaml.
# On macOS, index 0 uses AVFoundation for built-in MacBook camera. See docs/MACBOOK_LOW_LIGHT_VIDEO.md.
# CAMERA_SOURCES=auto
CAMERA_SOURCES=0
# Map (Leaflet/OSM): tile URL and default center when no site map_url. Frontend uses GET /api/v1/config/public.
# MAP_TILE_URL=https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png
# MAP_DEFAULT_LAT=51.505
# MAP_DEFAULT_LON=-0.09
# MAP_DEFAULT_ZOOM=13
# CONFIG_DIR=   # override config directory (config.json and optional cameras.yaml)
# RTSP: optional reconnection (seconds) and read timeout (ms) for unstable streams. See docs/TECHNOLOGY_RESEARCH_AND_CONFIG.md.
# RTSP_RECONNECT_SEC=15
# RTSP_TIMEOUT_MS=0

# Optional: low-light and clarity for laptop/MacBook. See docs/MACBOOK_LOW_LIGHT_VIDEO.md.
# ENHANCE_PRESET=macbook_air   # MacBook Air (stronger gamma/CLAHE in dim light)
# ENHANCE_VIDEO=1
# ENHANCE_LOW_LIGHT=0
# ENHANCE_CLARITY=0
# ENHANCE_CLAHE_CLIP=2.0
# ENHANCE_GAMMA=1.2

# PTZ: use GPIO on Pi/Jetson (set 1 to enable on non-ARM)
USE_GPIO=0
# Or use ONVIF camera for PTZ
# ONVIF_HOST=192.168.1.100
# ONVIF_PORT=80
# ONVIF_USER=admin
# ONVIF_PASS=password
# ONVIF_DISCOVERY_TIMEOUT_SEC=5   # timeout for ONVIF discovery (e.g. Detect cameras / API)

# Optional: disable audio / Wi-Fi analysis
# ENABLE_AUDIO=1
# Speech backend: google (cloud), vosk (offline), whisper (local). See docs/TECHNOLOGY_RESEARCH_AND_CONFIG.md.
# SPEECH_BACKEND=google
# VOSK_MODEL_PATH=   # path to Vosk model dir when SPEECH_BACKEND=vosk
# WHISPER_MODEL=base # tiny|base|small|medium|large when SPEECH_BACKEND=whisper
# ENABLE_WIFI_SNIFF=0
# WIFI_INTERFACE=wlan0mon
# Civilian / privacy: when set, only admin can export data and recordings (operator export blocked). See docs/CIVILIAN_ETHICS_AUDIT_AND_FEATURES.md.
# EXPORT_REQUIRES_APPROVAL=0
# Optional ReID (persistent identity); treat as biometric in some jurisdictions. Default off.
# ENABLE_REID=0
# Live predictive threat (proactive.predictor rule_based_threat) in analyze_frame. Set 1 to enable.
# ENABLE_PREDICTIVE_THREAT=0
# Auto PTZ: follow largest person in frame (ONVIF only). Cooldown 3 s between moves.
# AUTO_PTZ_FOLLOW=0
# Perimeter breach action: POST to URL and/or set GPIO high on line_cross/loitering (see LEGAL_AND_ETHICS.md).
# PERIMETER_ACTION_URL=
# ALERT_GPIO_PIN=0
# PERIMETER_GPIO_DURATION_SEC=5

# Autonomous action: POST to URL when threat_score >= threshold and event in list (e.g. lock API). Opt-in; high liability — see docs/LEGAL_AND_ETHICS.md.
# AUTONOMOUS_ACTION_URL=
# AUTONOMOUS_ACTION_THRESHOLD=70
# AUTONOMOUS_ACTION_EVENT_TYPES=line_cross,loitering

# Familiar vs stranger (watchlist): requires DeepFace. Set 1 to tag ai_data as watchlist name or Stranger; edge-only.
# ENABLE_WATCHLIST=0
# WATCHLIST_SIMILARITY_THRESHOLD=0.6

# Crowd density alert: when person count >= threshold, insert crowding event and trigger alert (0 = disabled).
# CROWD_DENSITY_ALERT_THRESHOLD=0
# Idle skip: when no motion for N seconds, sleep longer (ANALYZE_INTERVAL * multiplier) to save CPU. 0 = disabled.
# ANALYZE_IDLE_SKIP_SECONDS=0
# ANALYZE_IDLE_INTERVAL_MULTIPLIER=2

# AI data collection (only while recording is on). Batch size 1–50; interval 5–60 seconds.
# AI_DATA_BATCH_SIZE=10
# ANALYZE_INTERVAL_SECONDS=10

# Retention: delete ai_data, events, recordings older than N days (0 = disabled)
# RETENTION_DAYS=30

# Recordings and export path: where AVI/MP4 recordings and export files are saved. Leave unset for app directory, or set to an absolute path (e.g. external drive: /Volumes/MyDrive/recordings on macOS, /media/username/MyDrive on Linux). Can also be changed in Export → Storage location (admin).
# RECORDINGS_DIR=

# Alerts: POST to this URL with { phone, message } on motion/loitering/line_cross (e.g. Node /send-sms-alert)
# ALERT_SMS_URL=http://localhost:3000/send-sms-alert
# ALERT_PHONE=+1234567890
# Generic webhook: POST event JSON (event_type, camera_id, timestamp_utc, metadata, ...) to this URL
# ALERT_WEBHOOK_URL=https://your-server.com/webhook/vigil
# MQTT: publish events to broker (optional; requires paho-mqtt)
# ALERT_MQTT_BROKER=tcp://localhost:1883
# ALERT_MQTT_TOPIC=vms/events
# ALERT_MQTT_CLIENT_ID=vigil

# Serve React build at / when set to 1 (after: cd frontend && npm run build)
# USE_REACT_APP=1

# Optional: Redis for multi-instance WebSocket broadcast (scale-out)
# REDIS_URL=redis://localhost:6379/0

# ---- Presets (see docs/CONFIG_AND_OPTIMIZATION.md) ----
# Speed / low CPU: YOLO_IMGSZ=640 YOLO_CONF=0.35 STREAM_MAX_WIDTH=640 STREAM_JPEG_QUALITY=75 ANALYZE_INTERVAL_SECONDS=10
# Accuracy / small objects: YOLO_IMGSZ=1280 YOLO_CONF=0.25 STREAM_JPEG_QUALITY=88 ANALYZE_INTERVAL_SECONDS=5
# Jetson: JETSON_MODE=1 YOLO_MODEL=yolov8n.engine YOLO_TENSORRT_FP16=1 YOLO_IMGSZ=640
# Unstable RTSP: RTSP_RECONNECT_SEC=10 RTSP_TIMEOUT_MS=3000
# Production: gunicorn -w 4 -k gthread --threads 4; set FLASK_SECRET_KEY, SESSION_TIMEOUT_MINUTES=15, STRICT_TRANSPORT_SECURITY=1

# ---- Highest standards (1–100 rating: see docs/BEST_PATH_FORWARD_HIGHEST_STANDARDS.md) ----
# Applied project-wide: docs/STANDARDS_APPLIED.md. Optimized for production / evidence-grade:
# FLASK_SECRET_KEY=<long random>  SESSION_TIMEOUT_MINUTES=15  STRICT_TRANSPORT_SECURITY=1  ENFORCE_HTTPS=1
# SYSTEM_ID=site-01-dvr  ENABLE_RECORDING_FIXITY=1  ENABLE_EXTENDED_ATTRIBUTES=1  ENABLE_GAIT_NOTES=1
# YOLO_CONF=0.35  ANALYZE_INTERVAL_SECONDS=10  AI_DATA_BATCH_SIZE=10  RETENTION_DAYS=30
# EXPORT_REQUIRES_APPROVAL=1  (civilian mode: only admin can export)
# When using face/emotion or LPR: complete a DPIA and document lawful basis; for SAR/third-party sharing use a redaction tool on exported video (docs/RESEARCH_MILITARY_CIVILIAN_ACADEMIC_LE.md).
# Optional height calibration (Phase 2.4): reference height in cm and px so estimated_height_cm matches a known person at that pixel height (e.g. 170cm at 400px).
# HEIGHT_REF_CM=170
# HEIGHT_REF_PX=400
# Line-cross debounce: require centroid on opposite side for N cycles before firing (Phase 2.3; default 1).
# LINE_CROSS_DEBOUNCE_CYCLES=1
# Emotion: min person crop size in pixels for emotion inference (Phase 2.1; default 48; 224 for age/gender if using DeepFace).
# EMOTION_MIN_CROP_SIZE=48
# Emotion: CLAHE on L channel when mean intensity below this (Phase 2.1; default 80; 0=off). PLAN_90_PLUS.
# EMOTION_CLAHE_THRESHOLD=80
# Scene: max lower-half variance for Indoor (default 5000); higher = more Indoor when mean < 100. PLAN_90_PLUS.
# SCENE_VAR_MAX_INDOOR=5000
# HTTPS: 1/true=redirect to HTTPS; reject=return 403 (BEST_PATH_FORWARD Phase 3.2).
# ENFORCE_HTTPS=1
# Pose: min person crop size for MediaPipe (Phase 2.2; default 48). Pose label: Standing/Sitting/Walking from landmarks.
# POSE_MIN_CROP_SIZE=48
# Motion: backend framediff (default) or mog2 (DATA_POINT_ACCURACY_RATING; IEEE). MOTION_THRESHOLD=100-10000 (default 500).
# MOTION_MOG2_VAR_THRESHOLD=16 (4-64; PLAN_90_PLUS).
# MOTION_BACKEND=framediff
# MOTION_THRESHOLD=500
# Centroid smoothing for loiter/line-cross (PLAN_90_PLUS; 0=off, 5=default moving avg over 5 frames).
# CENTROID_SMOOTHING_FRAMES=5
